# ====== 核心连接配置 ======
debezium.sink.type=redis
debezium.sink.redis.address=redis:6379
debezium.sink.redis.password=${DEBEZIUM_REDIS_PASSWORD} # 从环境变量读取
debezium.sink.redis.key.prefix=cdc:${debezium.source.database.server.name}:  # 更清晰的命名空间
debezium.sink.redis.format=json  # 推荐：整个事件作为JSON字符串存入，兼容性最好
# 如果你想用Hash，明确指定：
# debezium.sink.redis.format=hash
# debezium.sink.redis.hash.key.prefix=hash:  # Hash结构专用前缀

debezium.source.connector.class=io.debezium.connector.postgresql.PostgresConnector
debezium.source.offset.storage.file.filename=data/offsets.dat
debezium.source.offset.flush.interval.ms=60000 # 生产环境建议增加，如60000ms(1分钟)
debezium.source.database.hostname=postgres
debezium.source.database.port=5432
debezium.source.database.user=${DEBEZIUM_DB_USER}
debezium.source.database.password=${DEBEZIUM_DB_PASSWORD}
debezium.source.database.dbname=${POSTGRES_DB}
debezium.source.database.server.name=pg-production-cluster-1 # 具有业务意义的名称

# ====== 闸门1: Schema与表过滤 ======
debezium.source.schema.include.list=public
debezium.source.table.include.list=public.users,public.orders,public.products
# 排除系统表
debezium.source.table.exclude.list=public.__debezium_.*

# ====== 闸门2: 快照控制 (关键!) ======
# initial: 首次启动做全量快照 (默认)
# when_needed: 仅在offset丢失时做
# never: 永远不做，仅接续binlog (适用于已有快照或测试)
# schema_only: 只同步表结构
debezium.source.snapshot.mode=initial
# 对超大表，可分批快照
debezium.source.snapshot.max.threads=2
debezium.source.snapshot.fetch.size=1024

# ====== 闸门3: 列与事件内容过滤 ======
# 列排除（敏感数据如密码不应流出）
debezium.source.column.exclude.list=public.users.password_hash,public.users.salt
# 基于行的事件过滤（例如，只同步 `status='active'` 的用户）
# 需自定义转换器，此处为配置示例
#debezium.source.transforms=filter
#debezium.source.transforms.filter.type=io.debezium.transforms.Filter
#debezium.source.transforms.filter.language=jsr223.groovy
#debezium.source.transforms.filter.condition=value.after.status == 'active'

# ====== 闸门4: 流量与性能控制 ======
debezium.source.max.batch.size=2048
debezium.source.max.queue.size=8192
debezium.source.poll.interval.ms=100
debezium.source.heartbeat.interval.ms=10000 # 保持offset推进，即使无数据

# ====== 闸门5: 事件路由 (SMTs) ======
# 添加主题路由，为未来引入Kafka留出接口
debezium.source.transforms=route
debezium.source.transforms.route.type=io.debezium.transforms.ByLogicalTableRouter
debezium.source.transforms.route.topic.regex=(.*)
debezium.source.transforms.route.topic.replacement=cdc_$1
debezium.source.transforms.route.key.field.regex=(.*)
debezium.source.transforms.route.key.field.replacement=cdc_$1

# ====== 闸门6: 死信队列 (DLQ) - 生产必备 ======
debezium.source.errors.log.enable=true
debezium.source.errors.log.include.messages=true
# 重试配置
debezium.source.errors.retry.delay.initial.ms=300
debezium.source.errors.retry.delay.max.ms=10000
debezium.source.errors.retry.max.attempts=10
# DLQ Topic (虽然你用的是Redis Sink，但配置应保留范式)
debezium.source.errors.deadletterqueue.topic.name=cdc_dlq

# ====== 监控与运维配置 ======
debezium.source.include.schema.changes=true
debezium.source.tombstones.on.delete=true # 保持true以正确传播删除事件
# 启用JMX监控
debezium.metrics.jmx.enabled=true
debezium.metrics.bootstrap.servers= # 若需推送到Kafka Metrics
# 暴露Prometheus指标 (如果未来集成)
#debezium.metrics.type=prometheus
#debezium.metrics.port=8080

# ====== 其他稳定化配置 ======
debezium.source.provide.transaction.metadata=true
debezium.source.time.precision.mode=adaptive
# 处理大字段（如图片、PDF元数据）
debezium.source.hstore.handling.mode=json
debezium.source.binary.handling.mode=base64
debezium.source.interval.handling.mode=string
